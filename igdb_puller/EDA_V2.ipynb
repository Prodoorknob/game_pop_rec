{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f55d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from igdb_puller import pull_games_and_dependents, pull_tables_as_globals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b464b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, re, time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FxOptions\n",
    "from selenium.webdriver.firefox.service import Service as FxService\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from urllib.parse import urlparse, urlunparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e90a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cricinfo_batch_firefox.py\n",
    "# ESPNcricinfo ball-by-ball scraper (Firefox)\n",
    "# - Resolves proper commentary URL via legacy redirect (+ strips '/full-scorecard')\n",
    "# - Uses YOUR absolute XPaths to select innings (1st/2nd) from the commentary page\n",
    "# - Loads every over via stepwise scrolling (tuned)\n",
    "# - Extracts ONLY commentary paragraphs (the \"red-box\" text)\n",
    "# - Exposes run_batch_from_df(...) for notebook use + CLI for scripting\n",
    "\n",
    "import os, csv, re, time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FxOptions\n",
    "from selenium.webdriver.firefox.service import Service as FxService\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "\n",
    "# -------------------------\n",
    "# GLOBAL CONFIG\n",
    "# -------------------------\n",
    "SCROLL_STEP_PX     = 100\n",
    "SCROLL_SETTLE_SEC  = 1.5\n",
    "GROWTH_PATIENCE    = 3\n",
    "MAX_SCROLL_ROUNDS  = 1000\n",
    "\n",
    "DEBUG = False  # flip True to print extra logs\n",
    "\n",
    "def dbg(*args):\n",
    "    if DEBUG:\n",
    "        print(\"[DBG]\", *args)\n",
    "\n",
    "BALL_RE = re.compile(r\"^\\s*(\\d+)\\.(\\d+)\\s*$\")\n",
    "\n",
    "# -------------------------\n",
    "# Consent / misc helpers\n",
    "# -------------------------\n",
    "def dismiss_consent_if_present(driver):\n",
    "    \"\"\"Best-effort click-through for consent banners; safe to no-op.\"\"\"\n",
    "    try:\n",
    "        buttons = driver.find_elements(\n",
    "            By.XPATH,\n",
    "            \"//button[contains(., 'Accept') or contains(., 'I Agree') or contains(., 'Continue') or contains(., 'Got it')]\"\n",
    "        )\n",
    "        for b in buttons:\n",
    "            if b.is_displayed() and b.is_enabled():\n",
    "                driver.execute_script(\"arguments[0].click();\", b)\n",
    "                time.sleep(0.5)\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# -------------------------\n",
    "# URL resolution / normalization\n",
    "# -------------------------\n",
    "def _strip_full_scorecard(url: str) -> str:\n",
    "    \"\"\"Remove '/full-scorecard' segment from any URL path.\"\"\"\n",
    "    p = urlparse(url)\n",
    "    path = p.path\n",
    "    path = path.replace(\"/full-scorecard/ball-by-ball-commentary\", \"/ball-by-ball-commentary\")\n",
    "    path = path.replace(\"/full-scorecard/live-cricket-score\", \"/live-cricket-score\")\n",
    "    path = path.replace(\"/full-scorecard/\", \"/\")\n",
    "    if path.endswith(\"/full-scorecard\"):\n",
    "        path = path[: -len(\"/full-scorecard\")]\n",
    "    return urlunparse((p.scheme, p.netloc, path, p.params, p.query, p.fragment))\n",
    "\n",
    "def resolve_commentary_url_via_redirect(match_id: str, driver) -> str:\n",
    "    \"\"\"\n",
    "    Use ESPN legacy engine URL to get the modern, fully-slugged match URL,\n",
    "    then normalize to /ball-by-ball-commentary and strip '/full-scorecard'.\n",
    "    \"\"\"\n",
    "    engine_url = f\"https://www.espncricinfo.com/ci/engine/match/{match_id}.html\"\n",
    "    driver.get(engine_url)\n",
    "    dismiss_consent_if_present(driver)\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    time.sleep(1.0)\n",
    "\n",
    "    final_url = _strip_full_scorecard(driver.current_url)\n",
    "    dbg(\"Resolved engine ->\", final_url)\n",
    "\n",
    "    if \"/ball-by-ball-commentary\" in final_url:\n",
    "        return final_url\n",
    "\n",
    "    if \"/live-cricket-score\" in final_url:\n",
    "        return _strip_full_scorecard(final_url.replace(\"/live-cricket-score\", \"/ball-by-ball-commentary\"))\n",
    "\n",
    "    parsed = urlparse(final_url)\n",
    "    if parsed.path.rstrip(\"/\").endswith(str(match_id)):\n",
    "        return _strip_full_scorecard(final_url.rstrip(\"/\") + \"/ball-by-ball-commentary\")\n",
    "\n",
    "    return _strip_full_scorecard(final_url.rstrip(\"/\") + \"/ball-by-ball-commentary\")\n",
    "\n",
    "# -------------------------\n",
    "# Commentary parser (ONLY the \"red-box\" text)\n",
    "# -------------------------\n",
    "def extract_rows_from_html(html: str):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    rows, seen = [], set()\n",
    "\n",
    "    def is_badge_text(s: str) -> bool:\n",
    "        s = s.strip()\n",
    "        return (\n",
    "            s in {\"â€¢\"} or\n",
    "            re.fullmatch(r\"\\d+[a-z]*\", s, flags=re.I) is not None or   # \"1\", \"1lb\", \"2nb\"\n",
    "            s.lower() in {\"lb\", \"nb\", \"wd\"}\n",
    "        )\n",
    "\n",
    "    def find_ball_container(node):\n",
    "        if isinstance(node, NavigableString):\n",
    "            node = node.parent\n",
    "        cur = node\n",
    "        for _ in range(8):\n",
    "            if cur is None:\n",
    "                break\n",
    "            try:\n",
    "                if cur.find(\"p\"):\n",
    "                    return cur\n",
    "            except Exception:\n",
    "                pass\n",
    "            cur = cur.parent\n",
    "        return node\n",
    "\n",
    "    # Iterate nodes that look like ball labels \"18.3\"\n",
    "    for txt_node in soup.find_all(string=BALL_RE):\n",
    "        label = txt_node.strip()\n",
    "        m = BALL_RE.match(label)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        container = find_ball_container(txt_node)\n",
    "        if container is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ps = [p.get_text(\" \", strip=True) for p in container.find_all(\"p\", recursive=True)]\n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "        ps = [p for p in ps if p and not is_badge_text(p)]\n",
    "\n",
    "        if not ps:\n",
    "            full = container.get_text(\" \", strip=True) or \"\"\n",
    "            full = re.sub(r\"^\\s*\"+re.escape(label)+r\"\\s*\", \"\", full)\n",
    "            if full and not is_badge_text(full):\n",
    "                ps = [full]\n",
    "\n",
    "        if not ps:\n",
    "            continue\n",
    "\n",
    "        text = \" \".join(ps).strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        over_i, ball_i = int(m.group(1)), int(m.group(2))\n",
    "        key = (label, text)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "\n",
    "        rows.append({\n",
    "            \"over\": over_i,\n",
    "            \"ball_in_over\": ball_i,\n",
    "            \"over_ball\": label,\n",
    "            \"text\": text,\n",
    "        })\n",
    "\n",
    "    rows.sort(key=lambda r: (r[\"over\"], r[\"ball_in_over\"]))\n",
    "    return rows\n",
    "\n",
    "# -------------------------\n",
    "# Scroller (stepwise; tuned)\n",
    "# -------------------------\n",
    "def load_all_by_scrolling(driver,\n",
    "                          step_px=SCROLL_STEP_PX,\n",
    "                          settle_sec=SCROLL_SETTLE_SEC,\n",
    "                          growth_patience=GROWTH_PATIENCE,\n",
    "                          max_rounds=MAX_SCROLL_ROUNDS):\n",
    "\n",
    "    def doc_heights():\n",
    "        return driver.execute_script(\"return [window.pageYOffset, document.body.scrollHeight, window.innerHeight];\")\n",
    "\n",
    "    def scroll_by(px):\n",
    "        driver.execute_script(\"window.scrollBy(0, arguments[0]);\", px)\n",
    "\n",
    "    def scroll_to_bottom_stepwise():\n",
    "        prev_y = -1\n",
    "        for _ in range(400):\n",
    "            y, h, vh = doc_heights()\n",
    "            if y >= h - vh - 2:\n",
    "                break\n",
    "            scroll_by(step_px)\n",
    "            time.sleep(0.15)\n",
    "            y2, _, _ = doc_heights()\n",
    "            if y2 == prev_y:\n",
    "                scroll_by(step_px)\n",
    "                time.sleep(0.15)\n",
    "                y2, _, _ = doc_heights()\n",
    "            prev_y = y2\n",
    "\n",
    "    def click_load_more_if_present():\n",
    "        candidates = driver.find_elements(\n",
    "            By.XPATH,\n",
    "            \"//button[contains(., 'Load more')] | //a[contains(., 'Load more')] | \"\n",
    "            \"//button[@aria-label='Load more' or @data-testid='load-more'] | \"\n",
    "            \"//a[@aria-label='Load more' or @data-testid='load-more']\"\n",
    "        )\n",
    "        btn = next((el for el in candidates if el.is_displayed() and el.is_enabled()), None)\n",
    "        if btn:\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                return True\n",
    "            except Exception:\n",
    "                try:\n",
    "                    btn.click()\n",
    "                    return True\n",
    "                except Exception:\n",
    "                    return False\n",
    "        return False\n",
    "\n",
    "    def unique_ball_count():\n",
    "        elems = driver.find_elements(By.XPATH, \"//*[normalize-space(text())]\")\n",
    "        s = set()\n",
    "        for e in elems:\n",
    "            try:\n",
    "                t = e.text.strip()\n",
    "                if BALL_RE.match(t):\n",
    "                    s.add(t)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return len(s)\n",
    "\n",
    "    last_count = -1\n",
    "    no_growth = 0\n",
    "\n",
    "    for _ in range(max_rounds):\n",
    "        scroll_to_bottom_stepwise()\n",
    "        time.sleep(settle_sec)\n",
    "\n",
    "        clicked = click_load_more_if_present()\n",
    "        if clicked:\n",
    "            time.sleep(settle_sec + 0.5)\n",
    "\n",
    "        # bottom jiggle to re-trigger observers\n",
    "        scroll_by(-200)\n",
    "        time.sleep(0.15)\n",
    "        scroll_by(400)\n",
    "        time.sleep(settle_sec)\n",
    "\n",
    "        cur = unique_ball_count()\n",
    "        dbg(\"Scroll round -> unique balls:\", cur)\n",
    "        if cur <= last_count:\n",
    "            no_growth += 1\n",
    "        else:\n",
    "            no_growth = 0\n",
    "            last_count = cur\n",
    "\n",
    "        if no_growth >= growth_patience:\n",
    "            break\n",
    "\n",
    "# -------------------------\n",
    "# YOUR absolute XPaths for innings dropdown + selectors\n",
    "# -------------------------\n",
    "# Button that opens the innings dropdown:\n",
    "XPATH_INNINGS_BUTTON = \"/html/body/div[1]/section/section/div[5]/div/div/div[3]/div[1]/div[2]/div[1]/div[1]/div/div[2]/div/div\"\n",
    "\n",
    "# Menu item for innings i (1 = first innings, 2 = second innings)\n",
    "def XPATH_INNINGS_ITEM(i:int) -> str:\n",
    "    # As per your working snippet (menu rendered under body/div[3])\n",
    "    return f\"/html/body/div[3]/div/div/div/div/div/ul/li[{i}]/div/span\"\n",
    "\n",
    "def open_innings_menu(driver, timeout=10):\n",
    "    WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    btn = WebDriverWait(driver, timeout).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, XPATH_INNINGS_BUTTON))\n",
    "    )\n",
    "    driver.execute_script(\"arguments[0].click();\", btn)\n",
    "\n",
    "def select_innings_by_index(driver, idx:int, timeout=10, settle=1.0) -> str:\n",
    "    \"\"\"Click the innings dropdown and select the idx-th item. Returns the label text (e.g., 'SRH').\"\"\"\n",
    "    open_innings_menu(driver, timeout=timeout)\n",
    "    item = WebDriverWait(driver, timeout).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, XPATH_INNINGS_ITEM(idx)))\n",
    "    )\n",
    "    label = (item.text or \"\").strip()\n",
    "    driver.execute_script(\"arguments[0].click();\", item)\n",
    "    time.sleep(settle)\n",
    "    return label or f\"Innings {idx}\"\n",
    "\n",
    "# -------------------------\n",
    "# One-match scrape (uses resolver + your innings clicks)\n",
    "# -------------------------\n",
    "def scrape_match(series_id: str, match_id: str, driver, innings_indices=(1,)) -> list:\n",
    "    \"\"\"\n",
    "    Scrape the specified innings indices (1=first, 2=second) for a match.\n",
    "    Returns list of dicts:\n",
    "      series_id, match_id, innings, over, ball_in_over, over_ball, text\n",
    "    \"\"\"\n",
    "    url = resolve_commentary_url_via_redirect(match_id, driver)\n",
    "    print(f\"[i] Resolved commentary URL for {match_id} -> {url}\")\n",
    "    driver.get(url)\n",
    "    dismiss_consent_if_present(driver)\n",
    "    driver.maximize_window()\n",
    "    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    time.sleep(1.0)\n",
    "\n",
    "    rows_all = []\n",
    "    for idx in innings_indices:\n",
    "        try:\n",
    "            innings_label = select_innings_by_index(driver, idx, timeout=10, settle=1.0)\n",
    "            print(f\"[i] Selected innings {idx} ({innings_label}) for {match_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Could not select innings {idx} for {match_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        load_all_by_scrolling(driver)\n",
    "        html = driver.page_source\n",
    "        rows = extract_rows_from_html(html)\n",
    "        for r in rows:\n",
    "            r[\"innings\"]   = innings_label or f\"Innings {idx}\"\n",
    "            r[\"match_id\"]  = match_id\n",
    "            r[\"series_id\"] = series_id\n",
    "        rows_all.extend(rows)\n",
    "\n",
    "    # De-dupe + sort\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for r in rows_all:\n",
    "        key = (r[\"innings\"], r[\"over_ball\"], r[\"text\"])\n",
    "        if key in seen: continue\n",
    "        seen.add(key)\n",
    "        unique.append(r)\n",
    "\n",
    "    unique.sort(key=lambda r: (r[\"over\"], r[\"ball_in_over\"]))\n",
    "    print(f\"[i] {match_id}: extracted {len(unique)} rows\")\n",
    "    return unique\n",
    "\n",
    "# -------------------------\n",
    "# Batch entry point for notebooks\n",
    "# -------------------------\n",
    "def run_batch_from_df(df,\n",
    "                      innings_indices=(1,2),\n",
    "                      out_dir=\"commentary_csv\",\n",
    "                      agg_csv=\"commentary_all.csv\",\n",
    "                      resume=False,\n",
    "                      headless=False,\n",
    "                      debug=False):\n",
    "    \"\"\"\n",
    "    df must contain columns: series_id, cricinfo_match_id\n",
    "    innings_indices: tuple/list of which innings to scrape via your XPath logic (1 and/or 2)\n",
    "    Writes per-match CSVs into out_dir/ and appends to agg_csv. Returns total rows appended.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # normalize column names (case-insensitive)\n",
    "    colmap = {}\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if cl in {\"series_id\", \"cricinfo_match_id\"}:\n",
    "            colmap[cl] = c\n",
    "    if set(colmap) != {\"series_id\", \"cricinfo_match_id\"}:\n",
    "        raise ValueError(\"DataFrame must have columns: series_id, cricinfo_match_id\")\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    agg_path = Path(agg_csv)\n",
    "    agg_exists = agg_path.exists()\n",
    "\n",
    "    agg_f = open(agg_path, \"a\" if agg_exists else \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "    agg_writer = csv.DictWriter(agg_f,\n",
    "        fieldnames=[\"series_id\",\"match_id\",\"innings\",\"over\",\"ball_in_over\",\"over_ball\",\"text\"])\n",
    "    if not agg_exists:\n",
    "        agg_writer.writeheader()\n",
    "\n",
    "    # one Firefox for the whole batch\n",
    "    opts = FxOptions()\n",
    "    if headless: opts.add_argument(\"-headless\")\n",
    "    service = FxService(GeckoDriverManager().install())\n",
    "    driver = webdriver.Firefox(service=service, options=opts)\n",
    "\n",
    "    global DEBUG\n",
    "    old_debug = DEBUG\n",
    "    DEBUG = bool(debug)\n",
    "\n",
    "    total_rows = 0\n",
    "    try:\n",
    "        for _, row in df.iterrows():\n",
    "            series_id = str(row[colmap[\"series_id\"]]).strip()\n",
    "            match_id  = str(row[colmap[\"cricinfo_match_id\"]]).strip()\n",
    "            if not series_id or not match_id:\n",
    "                continue\n",
    "\n",
    "            out_path = out_dir / f\"{match_id}_commentary.csv\"\n",
    "            if resume and out_path.exists():\n",
    "                print(f\"[i] Skipping {match_id} (exists, resume=True)\")\n",
    "                # keep agg in sync\n",
    "                with open(out_path, \"r\", encoding=\"utf-8\") as rf:\n",
    "                    for r in csv.DictReader(rf):\n",
    "                        agg_writer.writerow(r)\n",
    "                agg_f.flush()\n",
    "                continue\n",
    "\n",
    "            print(f\"[i] Scraping {match_id} â€¦\")\n",
    "            try:\n",
    "                rows_match = scrape_match(series_id, match_id, driver, innings_indices=innings_indices)\n",
    "            except Exception as e:\n",
    "                print(f\"[!] Failed {match_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "            with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as mf:\n",
    "                mw = csv.DictWriter(mf,\n",
    "                    fieldnames=[\"series_id\",\"match_id\",\"innings\",\"over\",\"ball_in_over\",\"over_ball\",\"text\"])\n",
    "                mw.writeheader()\n",
    "                for r in rows_match:\n",
    "                    mw.writerow(r)\n",
    "            print(f\"[i] Wrote {out_path} ({len(rows_match)} rows)\")\n",
    "\n",
    "            for r in rows_match:\n",
    "                agg_writer.writerow(r)\n",
    "            agg_f.flush()\n",
    "\n",
    "            total_rows += len(rows_match)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        agg_f.close()\n",
    "        DEBUG = old_debug\n",
    "\n",
    "    print(f\"[i] Done. Aggregate at: {agg_path} (+{total_rows} rows this run)\")\n",
    "    return total_rows\n",
    "\n",
    "# # -------------------------\n",
    "# # CLI\n",
    "# # -------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     import argparse, pandas as pd\n",
    "#     ap = argparse.ArgumentParser(description=\"Batch scrape ESPNcricinfo ball-by-ball commentary (Firefox)\")\n",
    "#     ap.add_argument(\"--index\", default=\"match_index_ipl.csv\", help=\"CSV with columns: series_id, cricinfo_match_id\")\n",
    "#     ap.add_argument(\"--outdir\", default=\"commentary_csv\")\n",
    "#     ap.add_argument(\"--agg\", default=\"commentary_all.csv\")\n",
    "#     ap.add_argument(\"--innings\", default=\"1,2\", help=\"Comma-separated innings indices to scrape, e.g. '1' or '1,2'\")\n",
    "#     ap.add_argument(\"--resume\", action=\"store_true\")\n",
    "#     ap.add_argument(\"--headless\", action=\"store_true\")\n",
    "#     ap.add_argument(\"--debug\", action=\"store_true\")\n",
    "#     args = ap.parse_args()\n",
    "\n",
    "#     innings_indices = tuple(int(x.strip()) for x in args.innings.split(\",\") if x.strip())\n",
    "\n",
    "#     df_idx = pd.read_csv(args.index)\n",
    "#     run_batch_from_df(\n",
    "#         df_idx,\n",
    "#         innings_indices=innings_indices,\n",
    "#         out_dir=args.outdir,\n",
    "#         agg_csv=args.agg,\n",
    "#         resume=args.resume,\n",
    "#         headless=args.headless,\n",
    "#         debug=args.debug,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6943167f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cricinfo_match_id</th>\n",
       "      <th>comp</th>\n",
       "      <th>season</th>\n",
       "      <th>series_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1082591</td>\n",
       "      <td>IPL</td>\n",
       "      <td>2017</td>\n",
       "      <td>1078425.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cricinfo_match_id comp  season  series_id\n",
       "0            1082591  IPL    2017  1078425.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {'cricinfo_match_id': [1082591],\n",
    "        'comp': ['IPL'],\n",
    "        'season': [2017],\n",
    "        'series_id': [1078425.0]}\n",
    "\n",
    "match_index = pd.DataFrame(data)\n",
    "display(match_index.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a627259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_idx = pd.read_csv(\"match_index_ipl.csv\")  # columns: series_id, cricinfo_match_id\n",
    "run_batch_from_df(\n",
    "    match_index,\n",
    "    innings_indices=(1,2),   # or (1,) to scrape only the first innings with your exact clicks\n",
    "    out_dir=\"commentary_csv\",\n",
    "    agg_csv=\"commentary_all.csv\",\n",
    "    resume=True,\n",
    "    headless=False,\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dismiss_consent_if_present(driver):\n",
    "    \"\"\"Best-effort click-through for consent banners; safe to no-op.\"\"\"\n",
    "    try:\n",
    "        buttons = driver.find_elements(\n",
    "            By.XPATH,\n",
    "            \"//button[contains(., 'Accept') or contains(., 'I Agree') or contains(., 'Continue') or contains(., 'Got it')]\"\n",
    "        )\n",
    "        for b in buttons:\n",
    "            if b.is_displayed() and b.is_enabled():\n",
    "                driver.execute_script(\"arguments[0].click();\", b)\n",
    "                time.sleep(0.5)\n",
    "                break\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f794939",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: Unable to locate element: /html/body/div[3]/div/div/div/div/div/ul/li[1]/div/span; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\nRemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\nWebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:202:5\nNoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:555:5\ndom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, value\u001b[38;5;241m=\u001b[39mb)\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     13\u001b[0m bb\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/html/body/div[3]/div/div/div/div/div/ul/li[1]/div/span\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, value\u001b[38;5;241m=\u001b[39mbb)\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     15\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     16\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "File \u001b[1;32mc:\\Users\\rajas\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:917\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot locate relative element with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elements[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENT, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\rajas\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:448\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    446\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    449\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\rajas\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: Unable to locate element: /html/body/div[3]/div/div/div/div/div/ul/li[1]/div/span; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\nRemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\nWebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:202:5\nNoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:555:5\ndom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n"
     ]
    }
   ],
   "source": [
    "opts = FxOptions()\n",
    "#if headless: opts.add_argument(\"-headless\")\n",
    "service = FxService(GeckoDriverManager().install())\n",
    "driver = webdriver.Firefox(service=service, options=opts)\n",
    "url = \"https://www.espncricinfo.com/series/ipl-2017-1078425/sunrisers-hyderabad-vs-royal-challengers-bangalore-1st-match-1082591/ball-by-ball-commentary\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "b=\"/html/body/div[1]/section/section/div[5]/div/div/div[3]/div[1]/div[2]/div[1]/div[1]/div/div[2]/div/div\"\n",
    "#dismiss_consent_if_present(driver)\n",
    "WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "time.sleep(1.0)\n",
    "driver.find_element(By.XPATH, value=b).click()\n",
    "bb=\"/html/body/div[3]/div/div/div/div/div/ul/li[1]/div/span\" \n",
    "driver.find_element(By.XPATH, value=bb).click()\n",
    "time.sleep(5)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adfff0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created df_games                            from 'games'  shape=(500, 56)\n",
      "Created df_game_time_to_beats               from 'game_time_to_beats'  shape=(219, 9)\n",
      "Created df_popularity_primitives            from 'popularity_primitives'  shape=(2439, 9)\n",
      "Created df_artworks                         from 'artworks'  shape=(858, 10)\n",
      "Created df_covers                           from 'covers'  shape=(500, 9)\n",
      "Created df_external_games                   from 'external_games'  shape=(1904, 15)\n",
      "Created df_game_localizations               from 'game_localizations'  shape=(282, 8)\n",
      "Created df_involved_companies               from 'involved_companies'  shape=(1566, 10)\n",
      "Created df_language_supports                from 'language_supports'  shape=(2541, 7)\n",
      "Created df_multiplayer_modes                from 'multiplayer_modes'  shape=(139, 14)\n",
      "Created df_release_dates                    from 'release_dates'  shape=(2679, 16)\n",
      "Created df_screenshots                      from 'screenshots'  shape=(2910, 9)\n",
      "Created df_game_videos                      from 'game_videos'  shape=(907, 5)\n",
      "Created df_websites                         from 'websites'  shape=(2483, 6)\n",
      "Created df_age_ratings                      from 'age_ratings'  shape=(1438, 9)\n",
      "Created df_genres                           from 'genres'  shape=(21, 7)\n",
      "Created df_keywords                         from 'keywords'  shape=(1862, 7)\n",
      "Created df_platforms                        from 'platforms'  shape=(80, 17)\n",
      "Created df_player_perspectives              from 'player_perspectives'  shape=(6, 7)\n",
      "Created df_game_modes                       from 'game_modes'  shape=(5, 7)\n",
      "Created df_franchises                       from 'franchises'  shape=(81, 8)\n",
      "Done pulling games and dependents.\n"
     ]
    }
   ],
   "source": [
    "pull_games_and_dependents(max_games=500, include_second_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7911845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_age_ratings (1438, 9)\n",
      "df_artworks (858, 10)\n",
      "df_covers (500, 9)\n",
      "df_external_games (1904, 15)\n",
      "df_franchises (81, 8)\n",
      "df_game_localizations (282, 8)\n",
      "df_game_modes (5, 7)\n",
      "df_game_time_to_beats (219, 9)\n",
      "df_game_videos (907, 5)\n",
      "df_games (500, 56)\n",
      "df_genres (21, 7)\n",
      "df_involved_companies (1566, 10)\n",
      "df_keywords (1862, 7)\n",
      "df_language_supports (2541, 7)\n",
      "df_multiplayer_modes (139, 14)\n",
      "df_platforms (80, 17)\n",
      "df_player_perspectives (6, 7)\n",
      "df_popularity_primitives (2439, 9)\n",
      "df_release_dates (2679, 16)\n",
      "df_screenshots (2910, 9)\n",
      "df_websites (2483, 6)\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted((k, v) for k, v in globals().items() if k.startswith('df_') and isinstance(v, pd.DataFrame)):\n",
    "    print(k, v.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
